{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'matplotlib'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
                    ]
                }
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import tensorflow as tf\n",
                "from sklearn.model_selection import train_test_split\n",
                "from tensorflow.keras import layers, Model, Sequential, losses, optimizers, metrics\n",
                "\n",
                "from helper import plot_df, resample\n",
                "from helper import decode_image, load_and_preprocess_image, load_tfrecord_dataset, parse_example\n",
                "from helper import load_compressed_model"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this coding exercise you will build a model that predicts 3 properties of an individual from a picture of their face: age, ethnicity, and gender.\n",
                "\n",
                "This Dataframe gives us an idea of what we are working with."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mRunning cells with '/Users/saket/miniconda3/envs/conda_ds/bin/python' requires the ipykernel package.\n",
                        "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
                        "\u001b[1;31mCommand: 'conda install -p /Users/saket/miniconda3/envs/conda_ds ipykernel --update-deps --force-reinstall'"
                    ]
                }
            ],
            "source": [
                "# Read the dataframe\n",
                "df = pd.read_csv('data/face_data.csv')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# map integer encodings to class labels\n",
                "# label ordering from the Kaggle dataset release:\n",
                "names_ethnicity = ['white', 'black', 'asian', 'indian', 'other']\n",
                "name_genders = ['male', 'female']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mRunning cells with '/Users/saket/miniconda3/envs/conda_ds/bin/python' requires the ipykernel package.\n",
                        "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
                        "\u001b[1;31mCommand: 'conda install -p /Users/saket/miniconda3/envs/conda_ds ipykernel --update-deps --force-reinstall'"
                    ]
                }
            ],
            "source": [
                "plot_df(df)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We'll use a helper function to `resample` to make sure the ethnicities are equally represented in a new Dataframe.\\\n",
                "Please don't change `OBSERVATION_PER_CLASS` from the default of 750."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mRunning cells with '/Users/saket/miniconda3/envs/conda_ds/bin/python' requires the ipykernel package.\n",
                        "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
                        "\u001b[1;31mCommand: 'conda install -p /Users/saket/miniconda3/envs/conda_ds ipykernel --update-deps --force-reinstall'"
                    ]
                }
            ],
            "source": [
                "OBSERVATIONS_PER_CLASS = 750\n",
                "df_resampled = resample(df, unbalanced_col='ethnicity', n_per_class=OBSERVATIONS_PER_CLASS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mRunning cells with '/Users/saket/miniconda3/envs/conda_ds/bin/python' requires the ipykernel package.\n",
                        "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
                        "\u001b[1;31mCommand: 'conda install -p /Users/saket/miniconda3/envs/conda_ds ipykernel --update-deps --force-reinstall'"
                    ]
                }
            ],
            "source": [
                "plot_df(df_resampled)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "That looks a little better, though not perfect. We'll settle for that for now."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Train / Validation / Test Split**\n",
                "\n",
                "Using your resampled data and `train_test_split`, create a test set consisting of 20% of the original dataset. Then set aside 10% of what was left over from the first split as a validation set. Whatever remains will be your train set.\n",
                "\n",
                "You should stratify your splits on both 'ethnicity' and 'gender' and use a random state of 109.\n",
                "\n",
                "Store the results in `train_df`, `val_df`, and `test_df`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mRunning cells with '/Users/saket/miniconda3/envs/conda_ds/bin/python' requires the ipykernel package.\n",
                        "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
                        "\u001b[1;31mCommand: 'conda install -p /Users/saket/miniconda3/envs/conda_ds ipykernel --update-deps --force-reinstall'"
                    ]
                }
            ],
            "source": [
                "### TEST FUNCTION: test_split\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "# Split the dataframe into train and validation sets\n",
                "# Create test set of 20% then set aside 10% of the training set for validation\n",
                "# Use random state of 109\n",
                "# Stratify on enthnicity and gender columns\n",
                "\n",
                "# split the data into train+validation and test sets\n",
                "train_val_df, test_df = train_test_split(resampled_df, test_size=0.2, random_state=109, stratify=resampled_df[['ethnicity', 'gender']])\n",
                "\n",
                "# split the train+validation set into train and validation sets\n",
                "train_df, val_df = train_test_split(train_val_df, test_size=0.1, random_state=109, stratify=train_val_df[['ethnicity', 'gender']])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_stratify\n",
                "# DO NOT REMOVE THE LINE ABOVE"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Creating Datasets**\n",
                "\n",
                "For the sake of consistency, to minimize mememory and storage space, and to make trainign as fast as possible, we will provide pre-made train, validation, and test datasets in the form of TF Dataset pipelines.\n",
                "\n",
                "These have been balanced for both ethnicity and gender within each ethnicity, similar to what you attempted above in the splitting.\n",
                "\n",
                "Run the code below to create train, validation, and test datasets. \n",
                "\n",
                "Later you may want to play with the batch size, but remember to stay conservative to prevent the kernel from dying."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 32 # num images in a generator batch\n",
                "IMG_DIM = (48, 48) # orignal images are 48x48 (model will require at least 32x32)\n",
                "train_tfrecord_file = 'data/faces_train_balanced.tfrecord'\n",
                "val_tfrecord_file = 'data/faces_val_balanced.tfrecord'\n",
                "test_tfrecord_file = 'data/faces_test_balanced.tfrecord'\n",
                "\n",
                "train_dataset = load_tfrecord_dataset(train_tfrecord_file, IMG_DIM, BATCH_SIZE)\n",
                "val_dataset = load_tfrecord_dataset(val_tfrecord_file, IMG_DIM, BATCH_SIZE)\n",
                "test_dataset = load_tfrecord_dataset(test_tfrecord_file, IMG_DIM, BATCH_SIZE)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can see what the datasets produce with this code:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "# inspect what the datasets produce\n",
                "train_dataset.element_spec"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The **inputs** are 48x48 grayscale images of faces \n",
                "(the single channel has been duplicated twice to make them 48x48x3 to work well with models trained on color images.)\n",
                "\n",
                "The **target** variables match the order of the columns in the Dataframe above.\n",
                "- The first target is the **age** variable (continous).\n",
                "- The second target is **ethnicity** (categorical)\n",
                "- The third target is **gender** (binary in this dataset)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Transfer Learning with BabyNet**\n",
                "\n",
                "The base of your model will be a very tiny CNN classifier pre-trained on the CFAR-10 dataset (~75% test accuracy). The model is 10% the size of MobileNetV3Small; We call it `BabyNet`. \n",
                "\n",
                "Load this pre-trained model from the `BabyNet.zip` file using the provided `load_compressed_model()` helper function, saving the model as `babynet`, and inspecting the summary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_babynet_load\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "# Split the dataframe into train and validation sets\n",
                "# Load the 'BabyNet' model\n",
                "babynet = load_compressed_model(...)\n",
                "babynet.summary()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Chop Off the Top!**\n",
                "\n",
                "We want to leverage the feature extraction ability of the convolutional section of the network (conv2d & pooling) since we think these features will generalize to our own task of predicting age, ethnicity, and gender from pictures of faces, (even though the CFAR-10 dataset contains no human faces!)\n",
                "\n",
                "But we *do not care* about the original 'top' of BabyNet since these final layers are more related to the specific CFAR-10 classification task.\n",
                "\n",
                "Create a new model, `base_model`, with the same `inputs` as `babynet` but whose outputs come from the final layer in the convolutional section of `babynet`. Be sure to also give your new model the name attribute 'base_model' when you construct it.\n",
                "\n",
                "Printing the summary is always helpful!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_choptop\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "# your code here\n",
                "base_model = ...\n",
                "\n",
                "base_model.summary()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Freeze Weights**\n",
                "\n",
                "In our initial round of training we don *not* want to mangle the pre-trained weights because we think they will be useful. So you should **set the base model's weight to be not trainable**.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_freeze\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "# your code here\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Data Augmentation**\n",
                "\n",
                "Create a small model to perform data augmentation. It should have one layer to perform a random, horizontal 'flip' followed by another layer to add a small amount of GaussianNoise. Store this model in the variable `data_augmentation` and give it the name attribute \"data_aug\".\n",
                "\n",
                "**Hint:**\n",
                "- See what layers are available in `tf.keras.layers`\n",
                "- It maybe be easier for you if you also use an input \"layer\"\n",
                "- Images are already normalized to be between 0 and 1 (what BabyNet expects) so the amount of noise you want to add should be very small."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_data_aug\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "# your code here\n",
                "data_augmentation = "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "# display data augmentation model summary\n",
                "data_augmentation.summary()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Run the code below to visualize some augmentation examples."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate variants of the first image from the training dataset\n",
                "for images, labels in train_dataset.take(1):\n",
                "    plt.figure(figsize=(4, 4))\n",
                "    first_image = images[0]\n",
                "    for i in range(4):\n",
                "        ax = plt.subplot(2, 2, i + 1)\n",
                "        # apply the transformation layers\n",
                "        augmented_image = data_augmentation(\n",
                "            tf.expand_dims(first_image, 0), training=True\n",
                "        )\n",
                "        plt.imshow(augmented_image[0])\n",
                "        plt.axis(\"off\")\n",
                "plt.tight_layout()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Building the Model**\n",
                "\n",
                "Construct your model. It's structure should look like this:\n",
                "- input \"layer\"\n",
                "- data augmentation\n",
                "- base_model\n",
                "- new intermediate layer(s)\n",
                "- 3 output layers, give them the name attributes 'age_output', 'ethnicity_output', 'gender_output'\n",
                "\n",
                "**Hint:**\n",
                "- The output of the base model is 3D (64 3x3 feature maps). Your new intermediate layer(s) will need too convert this output to 1D before it can be passed on to your output layers. We saw *at least 2 different ways* of doing this in class. Again, see what `tf.keras.layers` has to offer.\n",
                "- You can cause the kernel to crash (or just make your training very slow) if your model is too big. Start out small. You can pass all the tests in this notebook with less than 100k total parameters in your full network.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_output_count\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "# your code here\n",
                "model = ...\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_categorical_output\n",
                "# DO NOT REMOVE THE LINE ABOVE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_age_output\n",
                "# DO NOT REMOVE THE LINE ABOVE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_ethnicity_output\n",
                "# DO NOT REMOVE THE LINE ABOVE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_gender_output\n",
                "# DO NOT REMOVE THE LINE ABOVE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_data_aug_insert\n",
                "# DO NOT REMOVE THE LINE ABOVE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_base_model_insert\n",
                "# DO NOT REMOVE THE LINE ABOVE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "# display summary\n",
                "model.summary()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Compile the Model**\n",
                "\n",
                "When dealing with multiple outputs it is useful to pass dictionaries for both the `loss` and `metrics` arguments of `compile`. The dictionary keys are the layer names and their values are the losses or metrics themselves. Choose appropriate losses for each of the 3 outputs. You should monitor the following metrics:\n",
                "- age: mean absolute error\n",
                "- ethnicity: accuracy\n",
                "- gender: accuracy\n",
                "\n",
                "Create the dictionaries `loss` and `metrics` to pass to the `compile function`.\n",
                "\n",
                "**Note:** The values in the dictionaries should be *strings* representing the relevant loss or metric (Keras understands these). These strings are expected to be lowercase and any spaces should be replaced by underscores. You can also use abbreviations for some options which should still appease the test cases.\n",
                "\n",
                "For the **optimizer**, we recomment the Adam optimizer, though you might want to try something slighlty higher than the default learning rate.\n",
                "\n",
                "Save your optimizer as `optimizer1`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_optimizer1\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "# your code here\n",
                "optimizer1 = ...\n",
                "loss = ...\n",
                "metrics = ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_model_loss\n",
                "# DO NOT REMOVE THE LINE ABOVE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_model_metrics\n",
                "# DO NOT REMOVE THE LINE ABOVE"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Initial Training**\n",
                "\n",
                "Train the new layers you added to the base model. You'll want to do at least 3 epochs. More would likely help, but start small while you're still testing things."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_initial_training\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "# your code here\n",
                "initial_epochs = ...\n",
                "\n",
                "history1 = model.fit(train_dataset,\n",
                "                    epochs=initial_epochs,\n",
                "                    validation_data=val_dataset)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Unfreeze Base Model & Recompile with New Optimizer**\n",
                "\n",
                "Now we will fine-tune the base model. First you'll need to unfreeze its weights. Remember that you need to recompile the model for the change to take effect. When you print the summary the model should now show many more trainable parameters in the summary.\n",
                "\n",
                "When recompiling in preparation for fine tuning you should choose a sensible **learning rate** for the new optimizer. Call this new optimizer, `optimizer2`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_unfreeze\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "# unfreeze the base model\n",
                "# your code here\n",
                "optimizer2 = ...\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_optimizer2\n",
                "# DO NOT REMOVE THE LINE ABOVE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "# display summary\n",
                "model.summary()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Fine-Tuning**\n",
                "\n",
                "Now that *all* the weights in the network are trainable, train your model for a few more epochs. You'll probably want to do at least 2, but more may be beneficial depending on your initial training and model architecture.\n",
                "\n",
                "**Hint:** remember that adjusting the weights of the pre-trained model also has the potential to be *destructive* if the learning rate is too high."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_fine_tuning\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "# your code here\n",
                "fine_tune_epochs = ...\n",
                "total_epochs = initial_epochs + fine_tune_epochs\n",
                "\n",
                "history2 = model.fit(train_dataset,\n",
                "                    initial_epoch=initial_epochs,  # Resume training\n",
                "                    epochs=total_epochs,\n",
                "                    validation_data=val_dataset)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Display Full Training History**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_history = {}\n",
                "for key in history1.history.keys():\n",
                "    full_history[key] = history1.history[key] + history2.history[key]\n",
                "\n",
                "fig, axs = plt.subplots(1,3, figsize=(9,4))\n",
                "axs = axs.flatten()\n",
                "\n",
                "axs[0].plot(full_history['val_age_output_'+metrics['age_output']], label='train')\n",
                "axs[0].plot(full_history['age_output_'+metrics['age_output']], label='validation')\n",
                "axs[0].set_ylabel('MAE')\n",
                "axs[0].set_title('Age')\n",
                "\n",
                "axs[1].plot(full_history['ethnicity_output_'+metrics['ethnicity_output']], label='train')\n",
                "axs[1].plot(full_history['val_ethnicity_output_'+metrics['ethnicity_output']], label='validation')\n",
                "axs[1].axhline(0.2, c='r', ls=':', label='random chance')\n",
                "axs[1].set_ylabel('Accuracy')\n",
                "axs[1].set_title('Ethnicity')\n",
                "\n",
                "axs[2].plot(full_history['gender_output_'+metrics['gender_output']], label='train')\n",
                "axs[2].plot(full_history['val_gender_output_'+metrics['gender_output']], label='validation')\n",
                "axs[2].axhline(0.5, c='r', ls=':', label='random chance')\n",
                "axs[2].set_ylabel('Accuracy')\n",
                "axs[2].set_title('Gender')\n",
                "\n",
                "for ax in axs:\n",
                "    ax.set_xlabel(\"Epoch\")\n",
                "    ax.set_xticks(list(range(total_epochs))[::3])\n",
                "    x = np.arange(initial_epochs-1, total_epochs-1, 0.1)\n",
                "    ax.fill_between(x, *ax.get_ylim(), alpha=0.2, color='green', label='fine-tuning')\n",
                "    ax.legend();\n",
                "    \n",
                "plt.tight_layout()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Evaluate on Test Dataset**\n",
                "\n",
                "Evaluate your final model on test dataset and store the results in `test_eval`.\n",
                "\n",
                "You should be able to achieve:\n",
                "- Age MAE < 14\n",
                "- Ethnicity Accuracy > 0.3\n",
                "- Gender Accuracy > 0.6"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_test_eval\n",
                "# DO NOT REMOVE THE LINE ABOVE\n",
                "# your code here\n",
                "test_eval = \n",
                "print(f\"Test Age MAE: {test_eval[4]:.2f}\")\n",
                "print(f\"Test Ethnicity ACC: {test_eval[5]:.2f}\")\n",
                "print(f\"Test Gender ACC: {test_eval[6]:.2f}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Predict on a Test Batch**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Take one batch from the test dataset and store it as `test_batch`.\n",
                "\n",
                "Next, use your model to predict on this batch and store the predictions in `y_pred`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_y_pred\n",
                "# DO NOT REMOVE THE LINE \n",
                "test_batch = ...\n",
                "y_pred = ..."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### There is no more code for you to enter after this point. If your notebook runs to the end you pass the final test!\n",
                "**Processing Prediction Vectors**\n",
                "\n",
                "Here we create vectors `age_pred`, `eth_pred`, and `gen_pred` each containing the predicted values of the 3 target variables in the test batch.\n",
                "\n",
                "These vectors are all be flat (1D) and they should all contain *integers*."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "age_pred = np.round(y_pred[0].flatten()).astype(int)\n",
                "eth_pred = y_pred[1].argmax(axis=1)\n",
                "gen_pred = (y_pred[2].flatten() >= 0.5).astype(int)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Use this code to extract the images and target variables from the batch. Note the shapes of these objects. You may want to investigate them further."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "# input images and targets from batch\n",
                "images, labels =  next(test_batch.as_numpy_iterator())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('images.shape:', images.shape)\n",
                "print('len(labels):', len(labels))\n",
                "print('labels[0].shape:', labels[0].shape)\n",
                "print('labels[1].shape:', labels[1].shape)\n",
                "print('labels[2].shape:', labels[2].shape)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**True Labels**\n",
                "\n",
                "Here are the true vectors `age_true`, `eth_true`, and `gen_true` each containing the true values of the 3 target variables in the test batch.\n",
                "\n",
                "These vectors are all be flat (1D) and they all contain integers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "age_true, eth_true, gen_true = [y.flatten().astype(int) for y in labels]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Run the code below to inspect some example predictions from your validation batch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axs = plt.subplots(4,4, figsize=(16,16))\n",
                "axs = axs.flatten()\n",
                "for i in range(16):\n",
                "    axs[i].imshow(images[i])\n",
                "    pred_str = f'{age_pred[i]} {names_ethnicity[eth_pred[i]]} {name_genders[gen_pred[i]]}'\n",
                "    true_str = f'{age_true[i]} {names_ethnicity[eth_true[i]]} {name_genders[gen_true[i]]}'\n",
                "    axs[i].set_title('[PRED]\\n' + pred_str+'\\n'+'[TRUE]\\n'+ true_str)\n",
                "    axs[i].set_xticks([])\n",
                "    axs[i].set_yticks([])\n",
                "plt.tight_layout()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The results are probably not the greatest given the time and resource constrains, but with such a time model and dataset it is still surprising! "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "### TEST FUNCTION: test_you_did_it\n",
                "# DO NOT REMOVE THE LINE "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
