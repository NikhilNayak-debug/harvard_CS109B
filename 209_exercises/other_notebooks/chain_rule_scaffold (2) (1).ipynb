{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 422,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.metrics import mean_squared_error\n",
                "%matplotlib inline\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 423,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style=\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003ex\u003c/th\u003e\n      \u003cth\u003ey\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003e-5.00000\u003c/td\u003e\n      \u003ctd\u003e0.006693\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003e-4.98999\u003c/td\u003e\n      \u003ctd\u003e0.006760\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003e-4.97998\u003c/td\u003e\n      \u003ctd\u003e0.006827\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003e-4.96997\u003c/td\u003e\n      \u003ctd\u003e0.006895\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003e-4.95996\u003c/td\u003e\n      \u003ctd\u003e0.006964\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e",
                        "text/plain": "         x         y\n0 -5.00000  0.006693\n1 -4.98999  0.006760\n2 -4.97998  0.006827\n3 -4.96997  0.006895\n4 -4.95996  0.006964"
                    },
                    "execution_count": 423,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Get the data from the file `backprop.csv`\n",
                "df = pd.read_csv('backprop.csv')\n",
                "\n",
                "# Take a look at the data \n",
                "df.head()\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 424,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[[-5.        ]\n [-4.98998999]\n [-4.97997998]\n [-4.96996997]\n [-4.95995996]\n [-4.94994995]\n [-4.93993994]\n [-4.92992993]\n [-4.91991992]\n [-4.90990991]\n [-4.8998999 ]\n [-4.88988989]\n [-4.87987988]\n [-4.86986987]\n [-4.85985986]\n [-4.84984985]\n [-4.83983984]\n [-4.82982983]\n [-4.81981982]\n [-4.80980981]\n [-4.7997998 ]\n [-4.78978979]\n [-4.77977978]\n [-4.76976977]\n [-4.75975976]\n [-4.74974975]\n [-4.73973974]\n [-4.72972973]\n [-4.71971972]\n [-4.70970971]\n [-4.6996997 ]\n [-4.68968969]\n [-4.67967968]\n [-4.66966967]\n [-4.65965966]\n [-4.64964965]\n [-4.63963964]\n [-4.62962963]\n [-4.61961962]\n [-4.60960961]\n [-4.5995996 ]\n [-4.58958959]\n [-4.57957958]\n [-4.56956957]\n [-4.55955956]\n [-4.54954955]\n [-4.53953954]\n [-4.52952953]\n [-4.51951952]\n [-4.50950951]\n [-4.4994995 ]\n [-4.48948949]\n [-4.47947948]\n [-4.46946947]\n [-4.45945946]\n [-4.44944945]\n [-4.43943944]\n [-4.42942943]\n [-4.41941942]\n [-4.40940941]\n [-4.3993994 ]\n [-4.38938939]\n [-4.37937938]\n [-4.36936937]\n [-4.35935936]\n [-4.34934935]\n [-4.33933934]\n [-4.32932933]\n [-4.31931932]\n [-4.30930931]\n [-4.2992993 ]\n [-4.28928929]\n [-4.27927928]\n [-4.26926927]\n [-4.25925926]\n [-4.24924925]\n [-4.23923924]\n [-4.22922923]\n [-4.21921922]\n [-4.20920921]\n [-4.1991992 ]\n [-4.18918919]\n [-4.17917918]\n [-4.16916917]\n [-4.15915916]\n [-4.14914915]\n [-4.13913914]\n [-4.12912913]\n [-4.11911912]\n [-4.10910911]\n [-4.0990991 ]\n [-4.08908909]\n [-4.07907908]\n [-4.06906907]\n [-4.05905906]\n [-4.04904905]\n [-4.03903904]\n [-4.02902903]\n [-4.01901902]\n [-4.00900901]\n [-3.998999  ]\n [-3.98898899]\n [-3.97897898]\n [-3.96896897]\n [-3.95895896]\n [-3.94894895]\n [-3.93893894]\n [-3.92892893]\n [-3.91891892]\n [-3.90890891]\n [-3.8988989 ]\n [-3.88888889]\n [-3.87887888]\n [-3.86886887]\n [-3.85885886]\n [-3.84884885]\n [-3.83883884]\n [-3.82882883]\n [-3.81881882]\n [-3.80880881]\n [-3.7987988 ]\n [-3.78878879]\n [-3.77877878]\n [-3.76876877]\n [-3.75875876]\n [-3.74874875]\n [-3.73873874]\n [-3.72872873]\n [-3.71871872]\n [-3.70870871]\n [-3.6986987 ]\n [-3.68868869]\n [-3.67867868]\n [-3.66866867]\n [-3.65865866]\n [-3.64864865]\n [-3.63863864]\n [-3.62862863]\n [-3.61861862]\n [-3.60860861]\n [-3.5985986 ]\n [-3.58858859]\n [-3.57857858]\n [-3.56856857]\n [-3.55855856]\n [-3.54854855]\n [-3.53853854]\n [-3.52852853]\n [-3.51851852]\n [-3.50850851]\n [-3.4984985 ]\n [-3.48848849]\n [-3.47847848]\n [-3.46846847]\n [-3.45845846]\n [-3.44844845]\n [-3.43843844]\n [-3.42842843]\n [-3.41841842]\n [-3.40840841]\n [-3.3983984 ]\n [-3.38838839]\n [-3.37837838]\n [-3.36836837]\n [-3.35835836]\n [-3.34834835]\n [-3.33833834]\n [-3.32832833]\n [-3.31831832]\n [-3.30830831]\n [-3.2982983 ]\n [-3.28828829]\n [-3.27827828]\n [-3.26826827]\n [-3.25825826]\n [-3.24824825]\n [-3.23823824]\n [-3.22822823]\n [-3.21821822]\n [-3.20820821]\n [-3.1981982 ]\n [-3.18818819]\n [-3.17817818]\n [-3.16816817]\n [-3.15815816]\n [-3.14814815]\n [-3.13813814]\n [-3.12812813]\n [-3.11811812]\n [-3.10810811]\n [-3.0980981 ]\n [-3.08808809]\n [-3.07807808]\n [-3.06806807]\n [-3.05805806]\n [-3.04804805]\n [-3.03803804]\n [-3.02802803]\n [-3.01801802]\n [-3.00800801]\n [-2.997998  ]\n [-2.98798799]\n [-2.97797798]\n [-2.96796797]\n [-2.95795796]\n [-2.94794795]\n [-2.93793794]\n [-2.92792793]\n [-2.91791792]\n [-2.90790791]\n [-2.8978979 ]\n [-2.88788789]\n [-2.87787788]\n [-2.86786787]\n [-2.85785786]\n [-2.84784785]\n [-2.83783784]\n [-2.82782783]\n [-2.81781782]\n [-2.80780781]\n [-2.7977978 ]\n [-2.78778779]\n [-2.77777778]\n [-2.76776777]\n [-2.75775776]\n [-2.74774775]\n [-2.73773774]\n [-2.72772773]\n [-2.71771772]\n [-2.70770771]\n [-2.6976977 ]\n [-2.68768769]\n [-2.67767768]\n [-2.66766767]\n [-2.65765766]\n [-2.64764765]\n [-2.63763764]\n [-2.62762763]\n [-2.61761762]\n [-2.60760761]\n [-2.5975976 ]\n [-2.58758759]\n [-2.57757758]\n [-2.56756757]\n [-2.55755756]\n [-2.54754755]\n [-2.53753754]\n [-2.52752753]\n [-2.51751752]\n [-2.50750751]\n [-2.4974975 ]\n [-2.48748749]\n [-2.47747748]\n [-2.46746747]\n [-2.45745746]\n [-2.44744745]\n [-2.43743744]\n [-2.42742743]\n [-2.41741742]\n [-2.40740741]\n [-2.3973974 ]\n [-2.38738739]\n [-2.37737738]\n [-2.36736737]\n [-2.35735736]\n [-2.34734735]\n [-2.33733734]\n [-2.32732733]\n [-2.31731732]\n [-2.30730731]\n [-2.2972973 ]\n [-2.28728729]\n [-2.27727728]\n [-2.26726727]\n [-2.25725726]\n [-2.24724725]\n [-2.23723724]\n [-2.22722723]\n [-2.21721722]\n [-2.20720721]\n [-2.1971972 ]\n [-2.18718719]\n [-2.17717718]\n [-2.16716717]\n [-2.15715716]\n [-2.14714715]\n [-2.13713714]\n [-2.12712713]\n [-2.11711712]\n [-2.10710711]\n [-2.0970971 ]\n [-2.08708709]\n [-2.07707708]\n [-2.06706707]\n [-2.05705706]\n [-2.04704705]\n [-2.03703704]\n [-2.02702703]\n [-2.01701702]\n [-2.00700701]\n [-1.996997  ]\n [-1.98698699]\n [-1.97697698]\n [-1.96696697]\n [-1.95695696]\n [-1.94694695]\n [-1.93693694]\n [-1.92692693]\n [-1.91691692]\n [-1.90690691]\n [-1.8968969 ]\n [-1.88688689]\n [-1.87687688]\n [-1.86686687]\n [-1.85685686]\n [-1.84684685]\n [-1.83683684]\n [-1.82682683]\n [-1.81681682]\n [-1.80680681]\n [-1.7967968 ]\n [-1.78678679]\n [-1.77677678]\n [-1.76676677]\n [-1.75675676]\n [-1.74674675]\n [-1.73673674]\n [-1.72672673]\n [-1.71671672]\n [-1.70670671]\n [-1.6966967 ]\n [-1.68668669]\n [-1.67667668]\n [-1.66666667]\n [-1.65665666]\n [-1.64664665]\n [-1.63663664]\n [-1.62662663]\n [-1.61661662]\n [-1.60660661]\n [-1.5965966 ]\n [-1.58658659]\n [-1.57657658]\n [-1.56656657]\n [-1.55655656]\n [-1.54654655]\n [-1.53653654]\n [-1.52652653]\n [-1.51651652]\n [-1.50650651]\n [-1.4964965 ]\n [-1.48648649]\n [-1.47647648]\n [-1.46646647]\n [-1.45645646]\n [-1.44644645]\n [-1.43643644]\n [-1.42642643]\n [-1.41641642]\n [-1.40640641]\n [-1.3963964 ]\n [-1.38638639]\n [-1.37637638]\n [-1.36636637]\n [-1.35635636]\n [-1.34634635]\n [-1.33633634]\n [-1.32632633]\n [-1.31631632]\n [-1.30630631]\n [-1.2962963 ]\n [-1.28628629]\n [-1.27627628]\n [-1.26626627]\n [-1.25625626]\n [-1.24624625]\n [-1.23623624]\n [-1.22622623]\n [-1.21621622]\n [-1.20620621]\n [-1.1961962 ]\n [-1.18618619]\n [-1.17617618]\n [-1.16616617]\n [-1.15615616]\n [-1.14614615]\n [-1.13613614]\n [-1.12612613]\n [-1.11611612]\n [-1.10610611]\n [-1.0960961 ]\n [-1.08608609]\n [-1.07607608]\n [-1.06606607]\n [-1.05605606]\n [-1.04604605]\n [-1.03603604]\n [-1.02602603]\n [-1.01601602]\n [-1.00600601]\n [-0.995996  ]\n [-0.98598599]\n [-0.97597598]\n [-0.96596597]\n [-0.95595596]\n [-0.94594595]\n [-0.93593594]\n [-0.92592593]\n [-0.91591592]\n [-0.90590591]\n [-0.8958959 ]\n [-0.88588589]\n [-0.87587588]\n [-0.86586587]\n [-0.85585586]\n [-0.84584585]\n [-0.83583584]\n [-0.82582583]\n [-0.81581582]\n [-0.80580581]\n [-0.7957958 ]\n [-0.78578579]\n [-0.77577578]\n [-0.76576577]\n [-0.75575576]\n [-0.74574575]\n [-0.73573574]\n [-0.72572573]\n [-0.71571572]\n [-0.70570571]\n [-0.6956957 ]\n [-0.68568569]\n [-0.67567568]\n [-0.66566567]\n [-0.65565566]\n [-0.64564565]\n [-0.63563564]\n [-0.62562563]\n [-0.61561562]\n [-0.60560561]\n [-0.5955956 ]\n [-0.58558559]\n [-0.57557558]\n [-0.56556557]\n [-0.55555556]\n [-0.54554555]\n [-0.53553554]\n [-0.52552553]\n [-0.51551552]\n [-0.50550551]\n [-0.4954955 ]\n [-0.48548549]\n [-0.47547548]\n [-0.46546547]\n [-0.45545546]\n [-0.44544545]\n [-0.43543544]\n [-0.42542543]\n [-0.41541542]\n [-0.40540541]\n [-0.3953954 ]\n [-0.38538539]\n [-0.37537538]\n [-0.36536537]\n [-0.35535536]\n [-0.34534535]\n [-0.33533534]\n [-0.32532533]\n [-0.31531532]\n [-0.30530531]\n [-0.2952953 ]\n [-0.28528529]\n [-0.27527528]\n [-0.26526527]\n [-0.25525526]\n [-0.24524525]\n [-0.23523524]\n [-0.22522523]\n [-0.21521522]\n [-0.20520521]\n [-0.1951952 ]\n [-0.18518519]\n [-0.17517518]\n [-0.16516517]\n [-0.15515516]\n [-0.14514515]\n [-0.13513514]\n [-0.12512513]\n [-0.11511512]\n [-0.10510511]\n [-0.0950951 ]\n [-0.08508509]\n [-0.07507508]\n [-0.06506507]\n [-0.05505506]\n [-0.04504505]\n [-0.03503504]\n [-0.02502503]\n [-0.01501502]\n [-0.00500501]\n [ 0.00500501]\n [ 0.01501502]\n [ 0.02502503]\n [ 0.03503504]\n [ 0.04504505]\n [ 0.05505506]\n [ 0.06506507]\n [ 0.07507508]\n [ 0.08508509]\n [ 0.0950951 ]\n [ 0.10510511]\n [ 0.11511512]\n [ 0.12512513]\n [ 0.13513514]\n [ 0.14514515]\n [ 0.15515516]\n [ 0.16516517]\n [ 0.17517518]\n [ 0.18518519]\n [ 0.1951952 ]\n [ 0.20520521]\n [ 0.21521522]\n [ 0.22522523]\n [ 0.23523524]\n [ 0.24524525]\n [ 0.25525526]\n [ 0.26526527]\n [ 0.27527528]\n [ 0.28528529]\n [ 0.2952953 ]\n [ 0.30530531]\n [ 0.31531532]\n [ 0.32532533]\n [ 0.33533534]\n [ 0.34534535]\n [ 0.35535536]\n [ 0.36536537]\n [ 0.37537538]\n [ 0.38538539]\n [ 0.3953954 ]\n [ 0.40540541]\n [ 0.41541542]\n [ 0.42542543]\n [ 0.43543544]\n [ 0.44544545]\n [ 0.45545546]\n [ 0.46546547]\n [ 0.47547548]\n [ 0.48548549]\n [ 0.4954955 ]\n [ 0.50550551]\n [ 0.51551552]\n [ 0.52552553]\n [ 0.53553554]\n [ 0.54554555]\n [ 0.55555556]\n [ 0.56556557]\n [ 0.57557558]\n [ 0.58558559]\n [ 0.5955956 ]\n [ 0.60560561]\n [ 0.61561562]\n [ 0.62562563]\n [ 0.63563564]\n [ 0.64564565]\n [ 0.65565566]\n [ 0.66566567]\n [ 0.67567568]\n [ 0.68568569]\n [ 0.6956957 ]\n [ 0.70570571]\n [ 0.71571572]\n [ 0.72572573]\n [ 0.73573574]\n [ 0.74574575]\n [ 0.75575576]\n [ 0.76576577]\n [ 0.77577578]\n [ 0.78578579]\n [ 0.7957958 ]\n [ 0.80580581]\n [ 0.81581582]\n [ 0.82582583]\n [ 0.83583584]\n [ 0.84584585]\n [ 0.85585586]\n [ 0.86586587]\n [ 0.87587588]\n [ 0.88588589]\n [ 0.8958959 ]\n [ 0.90590591]\n [ 0.91591592]\n [ 0.92592593]\n [ 0.93593594]\n [ 0.94594595]\n [ 0.95595596]\n [ 0.96596597]\n [ 0.97597598]\n [ 0.98598599]\n [ 0.995996  ]\n [ 1.00600601]\n [ 1.01601602]\n [ 1.02602603]\n [ 1.03603604]\n [ 1.04604605]\n [ 1.05605606]\n [ 1.06606607]\n [ 1.07607608]\n [ 1.08608609]\n [ 1.0960961 ]\n [ 1.10610611]\n [ 1.11611612]\n [ 1.12612613]\n [ 1.13613614]\n [ 1.14614615]\n [ 1.15615616]\n [ 1.16616617]\n [ 1.17617618]\n [ 1.18618619]\n [ 1.1961962 ]\n [ 1.20620621]\n [ 1.21621622]\n [ 1.22622623]\n [ 1.23623624]\n [ 1.24624625]\n [ 1.25625626]\n [ 1.26626627]\n [ 1.27627628]\n [ 1.28628629]\n [ 1.2962963 ]\n [ 1.30630631]\n [ 1.31631632]\n [ 1.32632633]\n [ 1.33633634]\n [ 1.34634635]\n [ 1.35635636]\n [ 1.36636637]\n [ 1.37637638]\n [ 1.38638639]\n [ 1.3963964 ]\n [ 1.40640641]\n [ 1.41641642]\n [ 1.42642643]\n [ 1.43643644]\n [ 1.44644645]\n [ 1.45645646]\n [ 1.46646647]\n [ 1.47647648]\n [ 1.48648649]\n [ 1.4964965 ]\n [ 1.50650651]\n [ 1.51651652]\n [ 1.52652653]\n [ 1.53653654]\n [ 1.54654655]\n [ 1.55655656]\n [ 1.56656657]\n [ 1.57657658]\n [ 1.58658659]\n [ 1.5965966 ]\n [ 1.60660661]\n [ 1.61661662]\n [ 1.62662663]\n [ 1.63663664]\n [ 1.64664665]\n [ 1.65665666]\n [ 1.66666667]\n [ 1.67667668]\n [ 1.68668669]\n [ 1.6966967 ]\n [ 1.70670671]\n [ 1.71671672]\n [ 1.72672673]\n [ 1.73673674]\n [ 1.74674675]\n [ 1.75675676]\n [ 1.76676677]\n [ 1.77677678]\n [ 1.78678679]\n [ 1.7967968 ]\n [ 1.80680681]\n [ 1.81681682]\n [ 1.82682683]\n [ 1.83683684]\n [ 1.84684685]\n [ 1.85685686]\n [ 1.86686687]\n [ 1.87687688]\n [ 1.88688689]\n [ 1.8968969 ]\n [ 1.90690691]\n [ 1.91691692]\n [ 1.92692693]\n [ 1.93693694]\n [ 1.94694695]\n [ 1.95695696]\n [ 1.96696697]\n [ 1.97697698]\n [ 1.98698699]\n [ 1.996997  ]\n [ 2.00700701]\n [ 2.01701702]\n [ 2.02702703]\n [ 2.03703704]\n [ 2.04704705]\n [ 2.05705706]\n [ 2.06706707]\n [ 2.07707708]\n [ 2.08708709]\n [ 2.0970971 ]\n [ 2.10710711]\n [ 2.11711712]\n [ 2.12712713]\n [ 2.13713714]\n [ 2.14714715]\n [ 2.15715716]\n [ 2.16716717]\n [ 2.17717718]\n [ 2.18718719]\n [ 2.1971972 ]\n [ 2.20720721]\n [ 2.21721722]\n [ 2.22722723]\n [ 2.23723724]\n [ 2.24724725]\n [ 2.25725726]\n [ 2.26726727]\n [ 2.27727728]\n [ 2.28728729]\n [ 2.2972973 ]\n [ 2.30730731]\n [ 2.31731732]\n [ 2.32732733]\n [ 2.33733734]\n [ 2.34734735]\n [ 2.35735736]\n [ 2.36736737]\n [ 2.37737738]\n [ 2.38738739]\n [ 2.3973974 ]\n [ 2.40740741]\n [ 2.41741742]\n [ 2.42742743]\n [ 2.43743744]\n [ 2.44744745]\n [ 2.45745746]\n [ 2.46746747]\n [ 2.47747748]\n [ 2.48748749]\n [ 2.4974975 ]\n [ 2.50750751]\n [ 2.51751752]\n [ 2.52752753]\n [ 2.53753754]\n [ 2.54754755]\n [ 2.55755756]\n [ 2.56756757]\n [ 2.57757758]\n [ 2.58758759]\n [ 2.5975976 ]\n [ 2.60760761]\n [ 2.61761762]\n [ 2.62762763]\n [ 2.63763764]\n [ 2.64764765]\n [ 2.65765766]\n [ 2.66766767]\n [ 2.67767768]\n [ 2.68768769]\n [ 2.6976977 ]\n [ 2.70770771]\n [ 2.71771772]\n [ 2.72772773]\n [ 2.73773774]\n [ 2.74774775]\n [ 2.75775776]\n [ 2.76776777]\n [ 2.77777778]\n [ 2.78778779]\n [ 2.7977978 ]\n [ 2.80780781]\n [ 2.81781782]\n [ 2.82782783]\n [ 2.83783784]\n [ 2.84784785]\n [ 2.85785786]\n [ 2.86786787]\n [ 2.87787788]\n [ 2.88788789]\n [ 2.8978979 ]\n [ 2.90790791]\n [ 2.91791792]\n [ 2.92792793]\n [ 2.93793794]\n [ 2.94794795]\n [ 2.95795796]\n [ 2.96796797]\n [ 2.97797798]\n [ 2.98798799]\n [ 2.997998  ]\n [ 3.00800801]\n [ 3.01801802]\n [ 3.02802803]\n [ 3.03803804]\n [ 3.04804805]\n [ 3.05805806]\n [ 3.06806807]\n [ 3.07807808]\n [ 3.08808809]\n [ 3.0980981 ]\n [ 3.10810811]\n [ 3.11811812]\n [ 3.12812813]\n [ 3.13813814]\n [ 3.14814815]\n [ 3.15815816]\n [ 3.16816817]\n [ 3.17817818]\n [ 3.18818819]\n [ 3.1981982 ]\n [ 3.20820821]\n [ 3.21821822]\n [ 3.22822823]\n [ 3.23823824]\n [ 3.24824825]\n [ 3.25825826]\n [ 3.26826827]\n [ 3.27827828]\n [ 3.28828829]\n [ 3.2982983 ]\n [ 3.30830831]\n [ 3.31831832]\n [ 3.32832833]\n [ 3.33833834]\n [ 3.34834835]\n [ 3.35835836]\n [ 3.36836837]\n [ 3.37837838]\n [ 3.38838839]\n [ 3.3983984 ]\n [ 3.40840841]\n [ 3.41841842]\n [ 3.42842843]\n [ 3.43843844]\n [ 3.44844845]\n [ 3.45845846]\n [ 3.46846847]\n [ 3.47847848]\n [ 3.48848849]\n [ 3.4984985 ]\n [ 3.50850851]\n [ 3.51851852]\n [ 3.52852853]\n [ 3.53853854]\n [ 3.54854855]\n [ 3.55855856]\n [ 3.56856857]\n [ 3.57857858]\n [ 3.58858859]\n [ 3.5985986 ]\n [ 3.60860861]\n [ 3.61861862]\n [ 3.62862863]\n [ 3.63863864]\n [ 3.64864865]\n [ 3.65865866]\n [ 3.66866867]\n [ 3.67867868]\n [ 3.68868869]\n [ 3.6986987 ]\n [ 3.70870871]\n [ 3.71871872]\n [ 3.72872873]\n [ 3.73873874]\n [ 3.74874875]\n [ 3.75875876]\n [ 3.76876877]\n [ 3.77877878]\n [ 3.78878879]\n [ 3.7987988 ]\n [ 3.80880881]\n [ 3.81881882]\n [ 3.82882883]\n [ 3.83883884]\n [ 3.84884885]\n [ 3.85885886]\n [ 3.86886887]\n [ 3.87887888]\n [ 3.88888889]\n [ 3.8988989 ]\n [ 3.90890891]\n [ 3.91891892]\n [ 3.92892893]\n [ 3.93893894]\n [ 3.94894895]\n [ 3.95895896]\n [ 3.96896897]\n [ 3.97897898]\n [ 3.98898899]\n [ 3.998999  ]\n [ 4.00900901]\n [ 4.01901902]\n [ 4.02902903]\n [ 4.03903904]\n [ 4.04904905]\n [ 4.05905906]\n [ 4.06906907]\n [ 4.07907908]\n [ 4.08908909]\n [ 4.0990991 ]\n [ 4.10910911]\n [ 4.11911912]\n [ 4.12912913]\n [ 4.13913914]\n [ 4.14914915]\n [ 4.15915916]\n [ 4.16916917]\n [ 4.17917918]\n [ 4.18918919]\n [ 4.1991992 ]\n [ 4.20920921]\n [ 4.21921922]\n [ 4.22922923]\n [ 4.23923924]\n [ 4.24924925]\n [ 4.25925926]\n [ 4.26926927]\n [ 4.27927928]\n [ 4.28928929]\n [ 4.2992993 ]\n [ 4.30930931]\n [ 4.31931932]\n [ 4.32932933]\n [ 4.33933934]\n [ 4.34934935]\n [ 4.35935936]\n [ 4.36936937]\n [ 4.37937938]\n [ 4.38938939]\n [ 4.3993994 ]\n [ 4.40940941]\n [ 4.41941942]\n [ 4.42942943]\n [ 4.43943944]\n [ 4.44944945]\n [ 4.45945946]\n [ 4.46946947]\n [ 4.47947948]\n [ 4.48948949]\n [ 4.4994995 ]\n [ 4.50950951]\n [ 4.51951952]\n [ 4.52952953]\n [ 4.53953954]\n [ 4.54954955]\n [ 4.55955956]\n [ 4.56956957]\n [ 4.57957958]\n [ 4.58958959]\n [ 4.5995996 ]\n [ 4.60960961]\n [ 4.61961962]\n [ 4.62962963]\n [ 4.63963964]\n [ 4.64964965]\n [ 4.65965966]\n [ 4.66966967]\n [ 4.67967968]\n [ 4.68968969]\n [ 4.6996997 ]\n [ 4.70970971]\n [ 4.71971972]\n [ 4.72972973]\n [ 4.73973974]\n [ 4.74974975]\n [ 4.75975976]\n [ 4.76976977]\n [ 4.77977978]\n [ 4.78978979]\n [ 4.7997998 ]\n [ 4.80980981]\n [ 4.81981982]\n [ 4.82982983]\n [ 4.83983984]\n [ 4.84984985]\n [ 4.85985986]\n [ 4.86986987]\n [ 4.87987988]\n [ 4.88988989]\n [ 4.8998999 ]\n [ 4.90990991]\n [ 4.91991992]\n [ 4.92992993]\n [ 4.93993994]\n [ 4.94994995]\n [ 4.95995996]\n [ 4.96996997]\n [ 4.97997998]\n [ 4.98998999]\n [ 5.        ]]\n[[0.00669285]\n [0.00675973]\n [0.00682727]\n [0.00689548]\n [0.00696437]\n [0.00703394]\n [0.0071042 ]\n [0.00717515]\n [0.00724682]\n [0.00731919]\n [0.00739228]\n [0.00746609]\n [0.00754063]\n [0.00761592]\n [0.00769195]\n [0.00776873]\n [0.00784627]\n [0.00792458]\n [0.00800367]\n [0.00808353]\n [0.00816419]\n [0.00824565]\n [0.00832791]\n [0.00841099]\n [0.00849489]\n [0.00857961]\n [0.00866518]\n [0.00875159]\n [0.00883886]\n [0.00892698]\n [0.00901598]\n [0.00910586]\n [0.00919662]\n [0.00928828]\n [0.00938085]\n [0.00947433]\n [0.00956873]\n [0.00966407]\n [0.00976034]\n [0.00985757]\n [0.00995575]\n [0.0100549 ]\n [0.01015503]\n [0.01025614]\n [0.01035825]\n [0.01046137]\n [0.0105655 ]\n [0.01067066]\n [0.01077685]\n [0.01088409]\n [0.01099238]\n [0.01110174]\n [0.01121218]\n [0.0113237 ]\n [0.01143631]\n [0.01155004]\n [0.01166488]\n [0.01178085]\n [0.01189796]\n [0.01201621]\n [0.01213563]\n [0.01225622]\n [0.012378  ]\n [0.01250097]\n [0.01262514]\n [0.01275054]\n [0.01287716]\n [0.01300502]\n [0.01313414]\n [0.01326452]\n [0.01339618]\n [0.01352912]\n [0.01366337]\n [0.01379893]\n [0.01393582]\n [0.01407404]\n [0.01421362]\n [0.01435456]\n [0.01449687]\n [0.01464058]\n [0.01478569]\n [0.01493222]\n [0.01508018]\n [0.01522958]\n [0.01538043]\n [0.01553276]\n [0.01568657]\n [0.01584189]\n [0.01599871]\n [0.01615706]\n [0.01631695]\n [0.0164784 ]\n [0.01664142]\n [0.01680602]\n [0.01697223]\n [0.01714005]\n [0.01730949]\n [0.01748059]\n [0.01765334]\n [0.01782778]\n [0.0180039 ]\n [0.01818173]\n [0.01836128]\n [0.01854258]\n [0.01872563]\n [0.01891045]\n [0.01909706]\n [0.01928548]\n [0.01947572]\n [0.0196678 ]\n [0.01986173]\n [0.02005754]\n [0.02025523]\n [0.02045484]\n [0.02065637]\n [0.02085984]\n [0.02106528]\n [0.02127269]\n [0.0214821 ]\n [0.02169353]\n [0.02190699]\n [0.02212251]\n [0.0223401 ]\n [0.02255977]\n [0.02278156]\n [0.02300548]\n [0.02323154]\n [0.02345977]\n [0.02369019]\n [0.02392282]\n [0.02415768]\n [0.02439478]\n [0.02463416]\n [0.02487582]\n [0.02511979]\n [0.02536609]\n [0.02561474]\n [0.02586577]\n [0.02611919]\n [0.02637503]\n [0.0266333 ]\n [0.02689403]\n [0.02715725]\n [0.02742296]\n [0.02769121]\n [0.027962  ]\n [0.02823536]\n [0.02851132]\n [0.02878989]\n [0.02907111]\n [0.02935498]\n [0.02964155]\n [0.02993083]\n [0.03022284]\n [0.03051761]\n [0.03081516]\n [0.03111553]\n [0.03141872]\n [0.03172478]\n [0.03203371]\n [0.03234556]\n [0.03266033]\n [0.03297807]\n [0.03329879]\n [0.03362252]\n [0.03394929]\n [0.03427912]\n [0.03461204]\n [0.03494808]\n [0.03528726]\n [0.03562961]\n [0.03597516]\n [0.03632394]\n [0.03667596]\n [0.03703127]\n [0.03738989]\n [0.03775184]\n [0.03811715]\n [0.03848587]\n [0.038858  ]\n [0.03923358]\n [0.03961265]\n [0.03999523]\n [0.04038134]\n [0.04077102]\n [0.04116431]\n [0.04156122]\n [0.04196179]\n [0.04236606]\n [0.04277404]\n [0.04318577]\n [0.04360129]\n [0.04402062]\n [0.0444438 ]\n [0.04487086]\n [0.04530182]\n [0.04573672]\n [0.0461756 ]\n [0.04661848]\n [0.04706541]\n [0.0475164 ]\n [0.0479715 ]\n [0.04843073]\n [0.04889413]\n [0.04936174]\n [0.04983359]\n [0.0503097 ]\n [0.05079013]\n [0.05127489]\n [0.05176403]\n [0.05225758]\n [0.05275557]\n [0.05325804]\n [0.05376502]\n [0.05427655]\n [0.05479267]\n [0.05531341]\n [0.05583881]\n [0.05636889]\n [0.05690371]\n [0.05744329]\n [0.05798768]\n [0.0585369 ]\n [0.059091  ]\n [0.05965001]\n [0.06021398]\n [0.06078292]\n [0.0613569 ]\n [0.06193593]\n [0.06252007]\n [0.06310935]\n [0.0637038 ]\n [0.06430347]\n [0.06490839]\n [0.0655186 ]\n [0.06613414]\n [0.06675506]\n [0.06738138]\n [0.06801315]\n [0.06865041]\n [0.0692932 ]\n [0.06994155]\n [0.07059551]\n [0.07125511]\n [0.0719204 ]\n [0.07259142]\n [0.0732682 ]\n [0.07395079]\n [0.07463922]\n [0.07533355]\n [0.0760338 ]\n [0.07674002]\n [0.07745225]\n [0.07817053]\n [0.07889491]\n [0.07962541]\n [0.08036209]\n [0.08110499]\n [0.08185414]\n [0.08260959]\n [0.08337137]\n [0.08413954]\n [0.08491413]\n [0.08569519]\n [0.08648274]\n [0.08727685]\n [0.08807755]\n [0.08888487]\n [0.08969887]\n [0.09051958]\n [0.09134705]\n [0.09218131]\n [0.09302241]\n [0.0938704 ]\n [0.0947253 ]\n [0.09558718]\n [0.09645605]\n [0.09733198]\n [0.098215  ]\n [0.09910514]\n [0.10000246]\n [0.100907  ]\n [0.10181879]\n [0.10273788]\n [0.10366431]\n [0.10459811]\n [0.10553934]\n [0.10648803]\n [0.10744423]\n [0.10840796]\n [0.10937929]\n [0.11035824]\n [0.11134485]\n [0.11233918]\n [0.11334124]\n [0.1143511 ]\n [0.11536878]\n [0.11639433]\n [0.11742779]\n [0.11846919]\n [0.11951858]\n [0.12057599]\n [0.12164146]\n [0.12271504]\n [0.12379675]\n [0.12488664]\n [0.12598475]\n [0.12709111]\n [0.12820576]\n [0.12932874]\n [0.13046009]\n [0.13159983]\n [0.13274801]\n [0.13390467]\n [0.13506983]\n [0.13624354]\n [0.13742582]\n [0.13861672]\n [0.13981627]\n [0.1410245 ]\n [0.14224144]\n [0.14346712]\n [0.14470159]\n [0.14594487]\n [0.147197  ]\n [0.148458  ]\n [0.1497279 ]\n [0.15100675]\n [0.15229455]\n [0.15359136]\n [0.15489719]\n [0.15621207]\n [0.15753603]\n [0.1588691 ]\n [0.16021131]\n [0.16156268]\n [0.16292324]\n [0.164293  ]\n [0.16567201]\n [0.16706027]\n [0.16845783]\n [0.16986468]\n [0.17128087]\n [0.17270641]\n [0.17414131]\n [0.17558561]\n [0.17703932]\n [0.17850246]\n [0.17997505]\n [0.18145711]\n [0.18294864]\n [0.18444967]\n [0.18596022]\n [0.18748029]\n [0.1890099 ]\n [0.19054906]\n [0.19209779]\n [0.1936561 ]\n [0.19522399]\n [0.19680148]\n [0.19838857]\n [0.19998528]\n [0.2015916 ]\n [0.20320755]\n [0.20483313]\n [0.20646834]\n [0.20811319]\n [0.20976768]\n [0.21143182]\n [0.21310559]\n [0.214789  ]\n [0.21648206]\n [0.21818475]\n [0.21989708]\n [0.22161903]\n [0.22335061]\n [0.2250918 ]\n [0.22684261]\n [0.22860301]\n [0.23037301]\n [0.23215258]\n [0.23394173]\n [0.23574043]\n [0.23754867]\n [0.23936643]\n [0.24119371]\n [0.24303048]\n [0.24487672]\n [0.24673241]\n [0.24859754]\n [0.25047208]\n [0.25235601]\n [0.2542493 ]\n [0.25615193]\n [0.25806387]\n [0.25998509]\n [0.26191557]\n [0.26385527]\n [0.26580416]\n [0.26776221]\n [0.26972938]\n [0.27170565]\n [0.27369097]\n [0.2756853 ]\n [0.27768861]\n [0.27970086]\n [0.281722  ]\n [0.28375199]\n [0.28579078]\n [0.28783835]\n [0.28989462]\n [0.29195957]\n [0.29403313]\n [0.29611526]\n [0.2982059 ]\n [0.30030501]\n [0.30241253]\n [0.3045284 ]\n [0.30665257]\n [0.30878497]\n [0.31092556]\n [0.31307426]\n [0.31523102]\n [0.31739577]\n [0.31956845]\n [0.32174899]\n [0.32393732]\n [0.32613339]\n [0.32833711]\n [0.33054842]\n [0.33276724]\n [0.3349935 ]\n [0.33722713]\n [0.33946805]\n [0.34171618]\n [0.34397145]\n [0.34623378]\n [0.34850307]\n [0.35077927]\n [0.35306227]\n [0.355352  ]\n [0.35764837]\n [0.35995129]\n [0.36226068]\n [0.36457644]\n [0.36689849]\n [0.36922674]\n [0.37156109]\n [0.37390145]\n [0.37624773]\n [0.37859983]\n [0.38095765]\n [0.38332109]\n [0.38569007]\n [0.38806447]\n [0.3904442 ]\n [0.39282915]\n [0.39521923]\n [0.39761432]\n [0.40001433]\n [0.40241915]\n [0.40482867]\n [0.40724278]\n [0.40966139]\n [0.41208437]\n [0.41451162]\n [0.41694302]\n [0.41937848]\n [0.42181786]\n [0.42426107]\n [0.42670799]\n [0.4291585 ]\n [0.43161248]\n [0.43406983]\n [0.43653043]\n [0.43899415]\n [0.44146088]\n [0.44393051]\n [0.44640291]\n [0.44887797]\n [0.45135555]\n [0.45383556]\n [0.45631785]\n [0.45880232]\n [0.46128884]\n [0.46377728]\n [0.46626753]\n [0.46875947]\n [0.47125296]\n [0.47374789]\n [0.47624413]\n [0.47874155]\n [0.48124004]\n [0.48373947]\n [0.48623971]\n [0.48874064]\n [0.49124214]\n [0.49374407]\n [0.49624632]\n [0.49874875]\n [0.50125125]\n [0.50375368]\n [0.50625593]\n [0.50875786]\n [0.51125936]\n [0.51376029]\n [0.51626053]\n [0.51875996]\n [0.52125845]\n [0.52375587]\n [0.52625211]\n [0.52874704]\n [0.53124053]\n [0.53373247]\n [0.53622272]\n [0.53871116]\n [0.54119768]\n [0.54368215]\n [0.54616444]\n [0.54864445]\n [0.55112203]\n [0.55359709]\n [0.55606949]\n [0.55853912]\n [0.56100585]\n [0.56346957]\n [0.56593017]\n [0.56838752]\n [0.5708415 ]\n [0.57329201]\n [0.57573893]\n [0.57818214]\n [0.58062152]\n [0.58305698]\n [0.58548838]\n [0.58791563]\n [0.59033861]\n [0.59275722]\n [0.59517133]\n [0.59758085]\n [0.59998567]\n [0.60238568]\n [0.60478077]\n [0.60717085]\n [0.6095558 ]\n [0.61193553]\n [0.61430993]\n [0.61667891]\n [0.61904235]\n [0.62140017]\n [0.62375227]\n [0.62609855]\n [0.62843891]\n [0.63077326]\n [0.63310151]\n [0.63542356]\n [0.63773932]\n [0.64004871]\n [0.64235163]\n [0.644648  ]\n [0.64693773]\n [0.64922073]\n [0.65149693]\n [0.65376622]\n [0.65602855]\n [0.65828382]\n [0.66053195]\n [0.66277287]\n [0.6650065 ]\n [0.66723276]\n [0.66945158]\n [0.67166289]\n [0.67386661]\n [0.67606268]\n [0.67825101]\n [0.68043155]\n [0.68260423]\n [0.68476898]\n [0.68692574]\n [0.68907444]\n [0.69121503]\n [0.69334743]\n [0.6954716 ]\n [0.69758747]\n [0.69969499]\n [0.7017941 ]\n [0.70388474]\n [0.70596687]\n [0.70804043]\n [0.71010538]\n [0.71216165]\n [0.71420922]\n [0.71624801]\n [0.718278  ]\n [0.72029914]\n [0.72231139]\n [0.7243147 ]\n [0.72630903]\n [0.72829435]\n [0.73027062]\n [0.73223779]\n [0.73419584]\n [0.73614473]\n [0.73808443]\n [0.74001491]\n [0.74193613]\n [0.74384807]\n [0.7457507 ]\n [0.74764399]\n [0.74952792]\n [0.75140246]\n [0.75326759]\n [0.75512328]\n [0.75696952]\n [0.75880629]\n [0.76063357]\n [0.76245133]\n [0.76425957]\n [0.76605827]\n [0.76784742]\n [0.76962699]\n [0.77139699]\n [0.77315739]\n [0.7749082 ]\n [0.77664939]\n [0.77838097]\n [0.78010292]\n [0.78181525]\n [0.78351794]\n [0.785211  ]\n [0.78689441]\n [0.78856818]\n [0.79023232]\n [0.79188681]\n [0.79353166]\n [0.79516687]\n [0.79679245]\n [0.7984084 ]\n [0.80001472]\n [0.80161143]\n [0.80319852]\n [0.80477601]\n [0.8063439 ]\n [0.80790221]\n [0.80945094]\n [0.8109901 ]\n [0.81251971]\n [0.81403978]\n [0.81555033]\n [0.81705136]\n [0.81854289]\n [0.82002495]\n [0.82149754]\n [0.82296068]\n [0.82441439]\n [0.82585869]\n [0.82729359]\n [0.82871913]\n [0.83013532]\n [0.83154217]\n [0.83293973]\n [0.83432799]\n [0.835707  ]\n [0.83707676]\n [0.83843732]\n [0.83978869]\n [0.8411309 ]\n [0.84246397]\n [0.84378793]\n [0.84510281]\n [0.84640864]\n [0.84770545]\n [0.84899325]\n [0.8502721 ]\n [0.851542  ]\n [0.852803  ]\n [0.85405513]\n [0.85529841]\n [0.85653288]\n [0.85775856]\n [0.8589755 ]\n [0.86018373]\n [0.86138328]\n [0.86257418]\n [0.86375646]\n [0.86493017]\n [0.86609533]\n [0.86725199]\n [0.86840017]\n [0.86953991]\n [0.87067126]\n [0.87179424]\n [0.87290889]\n [0.87401525]\n [0.87511336]\n [0.87620325]\n [0.87728496]\n [0.87835854]\n [0.87942401]\n [0.88048142]\n [0.88153081]\n [0.88257221]\n [0.88360567]\n [0.88463122]\n [0.8856489 ]\n [0.88665876]\n [0.88766082]\n [0.88865515]\n [0.88964176]\n [0.89062071]\n [0.89159204]\n [0.89255577]\n [0.89351197]\n [0.89446066]\n [0.89540189]\n [0.89633569]\n [0.89726212]\n [0.89818121]\n [0.899093  ]\n [0.89999754]\n [0.90089486]\n [0.901785  ]\n [0.90266802]\n [0.90354395]\n [0.90441282]\n [0.9052747 ]\n [0.9061296 ]\n [0.90697759]\n [0.90781869]\n [0.90865295]\n [0.90948042]\n [0.91030113]\n [0.91111513]\n [0.91192245]\n [0.91272315]\n [0.91351726]\n [0.91430481]\n [0.91508587]\n [0.91586046]\n [0.91662863]\n [0.91739041]\n [0.91814586]\n [0.91889501]\n [0.91963791]\n [0.92037459]\n [0.92110509]\n [0.92182947]\n [0.92254775]\n [0.92325998]\n [0.9239662 ]\n [0.92466645]\n [0.92536078]\n [0.92604921]\n [0.9267318 ]\n [0.92740858]\n [0.9280796 ]\n [0.92874489]\n [0.92940449]\n [0.93005845]\n [0.9307068 ]\n [0.93134959]\n [0.93198685]\n [0.93261862]\n [0.93324494]\n [0.93386586]\n [0.9344814 ]\n [0.93509161]\n [0.93569653]\n [0.9362962 ]\n [0.93689065]\n [0.93747993]\n [0.93806407]\n [0.9386431 ]\n [0.93921708]\n [0.93978602]\n [0.94034999]\n [0.940909  ]\n [0.9414631 ]\n [0.94201232]\n [0.94255671]\n [0.94309629]\n [0.94363111]\n [0.94416119]\n [0.94468659]\n [0.94520733]\n [0.94572345]\n [0.94623498]\n [0.94674196]\n [0.94724443]\n [0.94774242]\n [0.94823597]\n [0.94872511]\n [0.94920987]\n [0.9496903 ]\n [0.95016641]\n [0.95063826]\n [0.95110587]\n [0.95156927]\n [0.9520285 ]\n [0.9524836 ]\n [0.95293459]\n [0.95338152]\n [0.9538244 ]\n [0.95426328]\n [0.95469818]\n [0.95512914]\n [0.9555562 ]\n [0.95597938]\n [0.95639871]\n [0.95681423]\n [0.95722596]\n [0.95763394]\n [0.95803821]\n [0.95843878]\n [0.95883569]\n [0.95922898]\n [0.95961866]\n [0.96000477]\n [0.96038735]\n [0.96076642]\n [0.961142  ]\n [0.96151413]\n [0.96188285]\n [0.96224816]\n [0.96261011]\n [0.96296873]\n [0.96332404]\n [0.96367606]\n [0.96402484]\n [0.96437039]\n [0.96471274]\n [0.96505192]\n [0.96538796]\n [0.96572088]\n [0.96605071]\n [0.96637748]\n [0.96670121]\n [0.96702193]\n [0.96733967]\n [0.96765444]\n [0.96796629]\n [0.96827522]\n [0.96858128]\n [0.96888447]\n [0.96918484]\n [0.96948239]\n [0.96977716]\n [0.97006917]\n [0.97035845]\n [0.97064502]\n [0.97092889]\n [0.97121011]\n [0.97148868]\n [0.97176464]\n [0.972038  ]\n [0.97230879]\n [0.97257704]\n [0.97284275]\n [0.97310597]\n [0.9733667 ]\n [0.97362497]\n [0.97388081]\n [0.97413423]\n [0.97438526]\n [0.97463391]\n [0.97488021]\n [0.97512418]\n [0.97536584]\n [0.97560522]\n [0.97584232]\n [0.97607718]\n [0.97630981]\n [0.97654023]\n [0.97676846]\n [0.97699452]\n [0.97721844]\n [0.97744023]\n [0.9776599 ]\n [0.97787749]\n [0.97809301]\n [0.97830647]\n [0.9785179 ]\n [0.97872731]\n [0.97893472]\n [0.97914016]\n [0.97934363]\n [0.97954516]\n [0.97974477]\n [0.97994246]\n [0.98013827]\n [0.9803322 ]\n [0.98052428]\n [0.98071452]\n [0.98090294]\n [0.98108955]\n [0.98127437]\n [0.98145742]\n [0.98163872]\n [0.98181827]\n [0.9819961 ]\n [0.98217222]\n [0.98234666]\n [0.98251941]\n [0.98269051]\n [0.98285995]\n [0.98302777]\n [0.98319398]\n [0.98335858]\n [0.9835216 ]\n [0.98368305]\n [0.98384294]\n [0.98400129]\n [0.98415811]\n [0.98431343]\n [0.98446724]\n [0.98461957]\n [0.98477042]\n [0.98491982]\n [0.98506778]\n [0.98521431]\n [0.98535942]\n [0.98550313]\n [0.98564544]\n [0.98578638]\n [0.98592596]\n [0.98606418]\n [0.98620107]\n [0.98633663]\n [0.98647088]\n [0.98660382]\n [0.98673548]\n [0.98686586]\n [0.98699498]\n [0.98712284]\n [0.98724946]\n [0.98737486]\n [0.98749903]\n [0.987622  ]\n [0.98774378]\n [0.98786437]\n [0.98798379]\n [0.98810204]\n [0.98821915]\n [0.98833512]\n [0.98844996]\n [0.98856369]\n [0.9886763 ]\n [0.98878782]\n [0.98889826]\n [0.98900762]\n [0.98911591]\n [0.98922315]\n [0.98932934]\n [0.9894345 ]\n [0.98953863]\n [0.98964175]\n [0.98974386]\n [0.98984497]\n [0.9899451 ]\n [0.99004425]\n [0.99014243]\n [0.99023966]\n [0.99033593]\n [0.99043127]\n [0.99052567]\n [0.99061915]\n [0.99071172]\n [0.99080338]\n [0.99089414]\n [0.99098402]\n [0.99107302]\n [0.99116114]\n [0.99124841]\n [0.99133482]\n [0.99142039]\n [0.99150511]\n [0.99158901]\n [0.99167209]\n [0.99175435]\n [0.99183581]\n [0.99191647]\n [0.99199633]\n [0.99207542]\n [0.99215373]\n [0.99223127]\n [0.99230805]\n [0.99238408]\n [0.99245937]\n [0.99253391]\n [0.99260772]\n [0.99268081]\n [0.99275318]\n [0.99282485]\n [0.9928958 ]\n [0.99296606]\n [0.99303563]\n [0.99310452]\n [0.99317273]\n [0.99324027]\n [0.99330715]]\n"
                }
            ],
            "source": [
                "# Assign the predictor and response variables to x and y\n",
                "# Ensure you reshape the data appropriately to convert vector to a matrix\n",
                "x = np.array(df.x).reshape(-1, 1)\n",
                "y = np.array(df.y).reshape(-1, 1)\n",
                "\n",
                "# Take a look at the predictor and response data post reshaping\n",
                "print(x)\n",
                "print(y)\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The network has 2 layers: a hidden layer and an output layer.\\\n",
                "Each layer has a single neuron (i.e., just one weight).\\\n",
                "$w_1$ is the weight for the 1st layer.\\\n",
                "$w_2$ is the weight for the 2nd layer.\\\n",
                "There are no bias terms.\n",
                "\n",
                "We will use randomly initialized weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 425,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the weights, keeping the random seed as 310 for reproducible results\n",
                "np.random.seed(310)\n",
                "\n",
                "# W is a list that contains both w1 and w2\n",
                "# That is, W = [w1,w2]\n",
                "W = [np.random.randn(1, 1), np.random.randn(1, 1)]\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Defining the activation function and the neural network"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 426,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_activation_fn) ###\n",
                "\n",
                "# Function to compute the sin activation function for backpropagation\n",
                "def A(x):\n",
                "    return np.sin(x)\n",
                "\n",
                "# Function to compute the derivative of the sin activation function\n",
                "def der_A(x):\n",
                "    return np.cos(x)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 427,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_forward) ###\n",
                "\n",
                "# Function to define the forward pass of neural network\n",
                "def neural_network(W, x):\n",
                "\n",
                "    # W is a list of the NN's two weights (w1,w2)\n",
                "    # x is the input to the neural network\n",
                "    w1 = W[0]\n",
                "    w2 = W[1]\n",
                "    \n",
                "    '''\n",
                "    Computes z1, z2, a1, and y_hat based on the image in the description\n",
                "    '''\n",
                "\n",
                "    z1 = x * w1\n",
                "    a1 = A(z1)\n",
                "    z2 = a1 * w2\n",
                "    y_hat = A(z2)\n",
                "    \n",
                "    return z1, z2, a1, y_hat\n",
                "    "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Building the chain-rule components \n",
                "Construct the individual partial derivatives required for finding the derivative of the loss w.r.t. each weight."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 428,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_dzdw) ###\n",
                "\n",
                "# Function to compute the partial derivate of affine transformation \n",
                "# wrt corresponding weight w\n",
                "\n",
                "def dzdw(layer_input):\n",
                "\n",
                "    '''\n",
                "    Take into account that this could be the first layer or an \n",
                "    intermediate layer\n",
                "    This function should be able to handle either case. \n",
                "    '''\n",
                "\n",
                "    return layer_input\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 429,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_dadz) ###\n",
                "\n",
                "# Function to compute the partial derivate of activation wrt affine\n",
                "\n",
                "def dadz(z):\n",
                "    \n",
                "    return der_A(z)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 430,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_dzda) ###\n",
                "\n",
                "# Function to compute the partial derivate of affine\n",
                "# wrt the previous activation\n",
                "\n",
                "def dzda(w):\n",
                "\n",
                "    return w\n",
                "    "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We use `y` rather than `y_hat` in the function names below just to keep things tidy (`dy_hatdz` would be more accurate but also very ugly)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 431,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_dydz) ###\n",
                "\n",
                "# Function to compute the partial derivate of output, y_hat, \n",
                "# wrt the affine\n",
                "\n",
                "def dydz(z):\n",
                "\n",
                "    return der_A(z)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 432,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_dldy) ###\n",
                "\n",
                "# Function to compute the partial derivate of loss with respect to y_hat\n",
                "# The loss used here is the squared error\n",
                "\n",
                "def dldy(y, y_hat):\n",
                "\n",
                "    '''\n",
                "    y is the grouth truth \n",
                "    y_hat is the predicted response\n",
                "    '''\n",
                "\n",
                "    return 2 * (y_hat-y)\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 433,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to compute the partial derivate of loss with respect to w\n",
                "\n",
                "def dldw(W, x, y):\n",
                "\n",
                "    '''\n",
                "    ARGS:\n",
                "    W: List of weights, w1 and w2\n",
                "    x: input for forward pass\n",
                "    y: true response variable\n",
                "\n",
                "    Combine the functions from above and find the derivative wrt the weights\n",
                "    These will be for all the points, hence take a mean of all values \n",
                "    for each partial derivative and return as a list of 2 values\n",
                "    '''\n",
                "\n",
                "    z1, z2, a1, y_hat = neural_network(W, x)\n",
                "    w1, w2 = W\n",
                "    \n",
                "    # Derivative of the loss wrt the second weight\n",
                "    # dldw2 = dldy(y, y_hat).reshape(-1,)*dydz(z2).reshape(-1,)*dzda(w2).reshape(-1,)[0]\n",
                "    dldw2 = dldy(y, y_hat)*dydz(z2)*dzdw(a1)\n",
                "\n",
                "    # print(\"dldy(y, y_hat)\", dldy(y, y_hat)[:5])\n",
                "    # print(\"dydz(z2)\", dydz(z2)[:5])\n",
                "    # print(\"dzdw(a1)\", dzdw(a1)[:5])\n",
                "\n",
                "\n",
                "    # Derivative of the loss wrt the first weight\n",
                "    dldw1 = dldy(y, y_hat)*dydz(z2)*dzda(w2)*dadz(z1)*dzdw(x)\n",
                "    \n",
                "    return [np.mean(dldw1), np.mean(dldw2)]\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 434,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "The derivatives of w1 and w2 wrt L are [-0.007777900562982069, 0.27880021231495045]\n"
                }
            ],
            "source": [
                "### edTest(test_gradient) ###\n",
                "\n",
                "# Use dldw() defined above to compute the gradient of the loss function \n",
                "# with respect to the weights\n",
                "gradW = dldw(W, x, y)\n",
                "# gradW = [0.02414, 0.27880]\n",
                "\n",
                "# Print the list of your gradients below\n",
                "print(f'The derivatives of w1 and w2 wrt L are {gradW}')\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 435,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 436,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 437,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
